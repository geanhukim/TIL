# 케라스 창시자에게 배우는 딥러닝
# 목차
- [1장 : 딥러닝이란 무엇인가?](#1장--딥러닝이란-무엇인가)
    - [1.1 인공 지능과 머신 러닝, 딥러닝](#11-인공-지능과-머신-러닝-딥러닝)
    - [1.2 딥러닝 이전: 머신 러닝의 간략한 역사](#12-딥러닝-이전-머신-러닝의-간략한-역사)
    - [1.3 왜 딥러닝일까? 왜 지금일까?](#13-왜-딥러닝일까-왜-지금일까)
- [2장 : 신경망의 수학적 구성요소](#2장--신경망의-수학적-구성요소)
    - [2.1 신경망과의 첫 만남](#21-신경망과의-첫-만남)
    - [2.2 신경망을 위한 데이터 표현](#22-신경망을-위한-데이터-표현)
    - [2.3 신경망의 톱니바퀴 : 텐서연산](#23-신경망의-톱니바퀴-텐서-연산)
    - [2.4 신경망의 엔진 : 그레이디언트 기반 최적화](#24-신경망의-엔진--그레이디언트-기반-최적화)
    - [2.5 첫 번째 예제 다시 살펴보기]
# 1장 : 딥러닝이란 무엇인가?
## 1.1 인공 지능과 머신 러닝, 딥러닝
### 인공지능
- '컴퓨터가 '생각'할 수 있는가?'라는 아이디어에서 시작
- 인공지능이란? 보통의 사람이 수행하는 지능적인 작업을 자동화하기 위한 연구 활동
- 머신러닝과 딥러닝을 포괄하는 종합 분야
- 심볼릭 AI : 명시적인 규칙을 충분히 만들어 db에 저장하여 지식을 다루는 방법
    - 1980년대까지 많이 쓰임
- 심볼릭 AI는 체스와 같이 잘 정의된 문제는 잘 풀었지만, 이미지 분류, 음성 인식과 같은 복잡하고 불분명한 문제를 해결하기 위한 정확한 규칙을 찾기 어려웠음 -> 머신 러닝 등장
### 머신 러닝
- 전통적인 프로그래밍에서는 프로그래머가 데이터를 적절한 해답으로 바꾸기 위해 따라야 하는 규칙을 작성함
- 머신러닝은 데이터와 이에 상응하는 해답을 보고 규칙을 찾아냄
### 데이터에서 표현을 학습하기
- 머신러닝을 하기 위해 필요한 것들
    - 입력 데이터 포인트
    - 기대 출력
    - 알고리즘의 성능을 측정하는 방법 : 현재 출력과 기대 출력 간 차이를 결정 => 학습
- ML/DL의 핵심 문제는 의미 있게 데이터를 변환하는 것 => 기대 출력에 가까워지도록 입력 데이터의 유용한 표현을 학습하는 것
- 표현 : 데이터를 인코딩하거나 표현하기 위해 데이터를 바라보는 방법
    - ex) 컬러 이미지를 RGB 포맷으로 볼 것인가, HSV 포맷으로 볼 것인가
### 딥러닝에서 '딥'이란 무엇일까?
- 딥러닝
    - 머신 러닝의 특정한 한 분야
    - 연속된 층(layer)에서 점진적으로 의미 있는 표현을 배우는데 강점
    - 딥러닝의 '딥' : 연속된 층으로 표현을 학습한다는 개념
- 신경망
    - 딥러닝이 사용하는 층이 겹겹이 쌓인 모델
### 딥러닝의 작동 원리 이해하기
- 가중치
    - 층이 입력 데이터를 처리하는 방식
    - 일련의 숫자로 이루어짐
    - 학습은 주어진 입력을 정확한 타깃에 매핑하기 위해 신경망의 모든 층에 있는 가중치의 값을 찾는 것
- 손실 함수(목적 함수, 비용 함수)
    - 신경망의 출력을 제어하기 위해 출력이 기대보다 얼머나 벗어났는지 측정
    - 신경망의 예측과 진짜 타깃의 차이를 점수로 계산
- 옵티마이저
    - 역전파 알고리즘을 사용하여 손실 점수가 감소되는 방향으로 가중치 값을 수정
- 훈련 반복
    - 초기에는 모델의 가중치가 랜덤값으로 할당
    - 자연스럽게 출력은 기대한 것과 달라지고, 손실점수가 매우 높음
    - 샘플 처리를 거듭하면서 가중치가 올바른 방향으로 조정, 손실 점수가 감소됨
### 지금까지 딥러닝의 성과
- 사람과 비슷한 수준의 이미지 분류, 음성 인식, 필기 인식
- 향상된 기계 번역, TTS 변환
- 디지털 비서
## 1.2 딥러닝 이전: 머신 러닝의 간략한 역사
### 확률적 모델링
- 통계학 이론을 데이터 분석에 응용
- 나이브 베이즈 알고리즘
    - 입력 데이터의 특성이 모두 독립적이라고 가정하고 베이즈 정리를 적용하는 머신 러닝 분류 알고리즘
- 로지스틱 회귀
### 초창기 신경망
- 1980년 대 중반부터 연구됨
- 경사 하강법 최적화를 사용하여 연쇄적으로 변수가 연결된 연산을 훈련하는 방법
### 커널 방법
- 분류 알고리즘의 한 종류
- 서포트 벡터 머신(SVM)
    - 두 클래스를 나누는 결정 경계를 찾는 분류 알고리즘
    - 과정
        - 결정 경계가 하나의 초평면으로 표현될 수 있는 고차원 표현으로 데이터를 매핑
        - 초평면과 각 클래스의 가장 가까운 데이터 포인트 사이의 거리가 최대가 되는 최선의 결정 경계를 찾음 -> 마진 최대화
### 결정 트리, 랜덤 포레스트, 그레이디언트 부스팅 머신
- 결정 트리
    - 입력 데이터 포인트를 분류하거나 주어진 입력에 대해 출력 값을 예측
- 랜덤 포레스터
    - 서로 다른 결정 트리를 많이 만들고 그 출력을 앙상블하는 방법
- 그레이디언트 부스팅 머신
    - 결정 트리를 앙상블하는 것을 기반으로 하는 머신 러닝 방법
    - 그레이디언트 부스팅 : 이전 모델에서 놓친 데이터 포인트를 보완하는 새로운 모델을 반복적으로 훈련하여 모델을 향상
### 다시 신경망으로
- 2011년 IDSIA에서 신층 신경망으로 학술 이미지 분류 대회에서 우승함으로써 시작
- 이후 심층 합성곱 신경망이 모든 컴퓨터 비전 작업의 주력 알고리즘으로 사용됨
### 딥러닝의 특징
- 딥러닝은 머신 러닝에서 가장 중요한 단계인 특성 공학을 완전히 자동화
- 특성 공학
    - 이전 머신 러닝 기법은 복잡한 문제를 풀 때 머신 러닝으로 처리하기 위해 입력 데이터를 수동으로 변환해야 함
    - 딥러닝은 직접 특성을 찾는 대신 한 번에 모든 특성을 학습함
- 
## 1.3 왜 딥러닝일까? 왜 지금일까?
# 2장 : 신경망의 수학적 구성요소
## 2.1 신경망과의 첫 만남
- MNIST 손글씨 숫자 이미지 분류
``` python
from tensorflow.keras.datasets import mnist
# 훈련 세트와 테스트 세트를 구성
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

from tensorflow import keras
from tensorflow.keras import layers
# 신경망 모델
model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(10, activation="softmax")
])

#컴파일 단계(옵티마이저 : 성능 향상을 위해 인풋을 기반으로 모델을 업데이트, 손실함수 : 모델의 성능을 측정, 모니터링 지표)
model.compile(optimizer="rmsprop", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

# 데이터를 모델에 맞는 크기로 조정
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype("float32") / 255

# 데이터에 모델을 학습
model.fit(train_images, train_labels, epochs=5, batch_size = 128)

# 모델 평가하기
test_loss, test_acc = model.evaluate(test_images, test_labels)
```
## 2.2 신경망을 위한 데이터 표현
- 텐서 : 데이터를 위한 컨테이너
---
### 스칼라(랭크-0 텐서)
- 하나의 숫자만 담고 있는 텐서
- 축 개수(랭크)가 0
```python
x = np.array(12)
x # array(12)
x.ndim # 0
```
---
### 벡터(랭크-1 텐서)
- 숫자의 배열
- 하나의 축을 가짐
```python
x = np.array([12, 3, 6, 14, 7])
x.ndim # 1
```
---
### 행렬(랭크-2 텐서)
- 벡터의 배열
- 2개의 축(행과 열)을 가짐
```python
x = np.array([
    [5, 78, 2, 34, 0],
    [6, 79, 3, 35, 1],
    [7, 89, 4, 36, 2]
])
x.ndim # 2
```
---
### 랭크-3 텐서와 더 높은 랭크의 텐서
- 행렬들을 하나의 배열로 합치면 랭크-3 텐서
- 랭크-3 텐서를 하나의 배열로 합치면 렝크-4 텐서
---
### 핵심 속성
- 축의 개수(랭크)
    - 랭크-3 텐서는 3개의 축, 행렬은 2개의 축
- 크기(shape)
    - 텐서의 각 축을 따라 얼마나 많은 차원이 있는지를 나타낸 파이썬의 튜플
    - 위의 행렬의 크기는 (3, 5), 벡터의 크기는 (5,), 스칼라는 ()
- 데이터 타입
    - 텐서의 포함된 데이터의 타입
---
### 넘파이로 텐서 조작하기
- 슬라이싱 : 배열에 있는 특정 원소들을 선택하는 것
---
### 배치 데이터
- 샘플 축 : 딥러닝에서 사용하는 모든 데이터 텐서의 첫 번째 축
---
### 텐서의 실제 사례
- 벡터 데이터
    - 첫 번째 축 : 샘플 축
    - 두 번째 축 : 특성 축
    - ex) 3개의 특성을 가진 10만명의 인구 통계 데이터 : (100000, 3)
- 시계열 또는 시퀀스 데이터
    - 시간 축을 포함하여 랭크-3 텐서로 저장
    - 시간 축은 관례적으로 두 번째 축
- 이미지 데이터
    - 관례상 랭크-3 텐서로 저장
    - ex) 128 개의 256 * 256크기의 컬러 이미지 : (128, 256, 256, 2)
- 비디오 데이터
    - 랭크-5 텐서
---
## 2.3 신경망의 톱니바퀴: 텐서 연산
### 원소별 연산
- 원소별로 덧셈, 뺄셈, 곱셈 등을 하는 것
- 넘파이 배열을 다룰 때 넘파이 내장함수로 더 빠르게 처리할 수 있음
---
### 브로드캐스팅
- 다른 랭크의 텐서끼리의 연산에서 작은 텐서를 큰 텐서의 크기에 맞추어 변환 후 계산하는 것
---
### 텐서 곱셈(점곱)
- `np.dot`함수를 사용
---
### 텐서 크기 변환
-  특정 크기에 맞게 열과 행을 재배열
---
### 텐서 연산의 기하학적 해석
- 텐서 연산이 조작하는 텐서의 내용은 기하학적 공간에 있는 좌표 포인트로 해석 가능
    - A = [0.5, 1], B = [1, 0.25]라는 두 벡터를 더하는 계산
        - 각 벡터는 2차원 평면상의 두 점으로 해석할 수 있음
        - 덧셈 계산은 점A를 새로운 위치로 복사하는 동작
        - 즉, 텐서 덧셈은 객체를 특정 방향으로 특정 양만큼 이동하는 행동을 나타냄
- 이동, 회전, 크기 변경, 기울이기 등과 같은 기본적인 기하학적 연산을 텐서 연산으로 표현 가능
    - 이동 : 점 집합에 벡터를 더해 고정된 방향으로 고정된 양만큼 이동시키는 것
    - 회전 : 각도 a만큼 벡터를 반시계 방향 회전할려면 벡터와 `[[cos(a), -sin(a)],[sin(a), cos(a)]]`를 점곱하여 얻을 수 있음
    - 크기 변경 : `[[수평값, 0], [0, 수직값]]`을 점곱하여 수직과 수평방향 크기를 변경할 수 있음
    - 선형 변환 : 임의의 행렬과 점곱하여 선형변환을 수행(변경, 회전이 해당)
    - 아핀 변환 : 선형 변환과 이동의 조합, 활성화 함수를 사용하지 않은 Dense층이 일종의 아핀 변환 층
---
### 딥러닝의 기하학적 해석
- 두 가지 색의 종이를 뭉쳐 종이공을 만든 후 종이를 분류해야 함
- 신경망은 종이 공을 조금씩 펼치는 것처럼 깔끔하게 펼치는 변환을 찾아냄
- 즉, 딥러닝은 기초적인 연산을 길게 연결하여 복잡한 기하학적 변환을 조금씩 분해함
- 딥러닝 신경망의 각 층들이 데이터를 조금씩 풀어 주고, 이 층들을 깊게 쌓아 복잡한 분해 과정을 처리함
---
## 2.4 신경망의 엔진 : 그레이디언트 기반 최적화
- 위 예제의 층은 입력 데이터를 `output = relu(dot(W, input) + b)`로 변환함
- w와 b를 가중치라고 부름
- 초기에는 가중치들이 난수로 채워져 있음, 훈련을 통해 점진적으로 조정됨
- 훈련 반복 루프
    - 샘플 x와 상응하는 y_true의 배치를 추출
    - x를 사용하여 모델 실행, 예측 y_pred를 구함
    - y_pred와 y_true의 차이를 측정하여 해당 배치에 대한 모델의 손시를 계산
    - 배치에 대한 손실이 감소하도록 모델의 모든 가중치를 업데이트
- 현대 신경망은 `경사 하강법`을 통해 가중치를 업데이트 함
---
### 도함수란?
- 미분가능한 y=f(x) 함수가 있음
- x를 조금 바꾸면 y도 조금만 변경됨
- x의 변화량이 작다면 어떤 점 p에서 선형함수로 f를 근사할 수 있음
    - ` f(x+x') = y + a * x'`
- 이 때 기울기 a를 p에서의 f의 도함수라고 함
- 도함수는 f(x)를 최소화하는 x값을 찾는 작업인 최적화에 도움을 줌
    - 도함수는 x가 바뀔 때 f(x)가 어떻게 바뀔지 설명해 주기 때문에
---
### 텐서 연산의 도함수 : 그레이디언트
- 그레이디언트 : 텐서 연산의 도함수
    - 위에서는 스칼라 값을 활용
    - 실제 딥러닝에선 랭크-2,3 등 다양한 텐서를 값을 받는 함수들이 존재
- 그레이디언트는 다차원 표면의 곡률을 나타냄
```python
# 입력 벡터 : x
# 모델의 가중치 행렬 : W
# 타깃, y_true
# 손실함수 : loss

y_pred = dot(W, x)
loss_value = loss(y_pred, y_true)

# 위의 연산은 W 값을 손실 값에 매핑하는 함수로 해설할 수 있음
# -> 가중치들의 값에 따라 손실값이 달라지기 떄문에
loss_value = f(W)

# 현재값이 W0일 때 f의 도함수
grad(loss_value, W0)

# 이 도함수의 각 원소들(j층에서 i층으로의 가중치)
# 해당 가중치만의 도함수를 반환
grad(loss_value, W0)[i,j]

# 결론
loss_value의 값을 줄이기 위해 f(W)를 그레이디언트의 반대방향으로 움직이면 됨
```
---
### 확률적 경사 하강법
- 미분가능 함수의 최솟값 = 도함수가 0인 지점 -> 도함수가 0이 되는 지점을 모두 찾아 함수 값이 가장 작은 포인트를 확인해야 함
- 딥러닝에서는 가장 작은 loss_value를 만드는 가중치의 조합 W를 찾는 것
- 미니 배치 확률적 경사 하강법
    - 확률적 : 각 배치 데이터가 무작위로 선택됨
    - 과정
        - 훈련 샘플 배치 x와 타깃 y_true를 추출
        - x로 모델을 실행, 예측 y_pred를 구함(정방향 패스)
        - y_pred와 y_true 사이의 오차를 측정하여 모델의 손실을 계산
        - 모델의 파라미터에 대한 손실 함수의 그레이디언트를 계산(역방향 패스)
        - 그레이디언트의 반대 방향으로 파라미터를 이동, 학습률(learning rate)에 따라 조절량이 결정
            - 학습률이 너무 작으면 지역 최솟값에 갇힐 수 있고, 너무 크면 곡선상에서 완전히 임위의 위치로 이동할 수 있음 -> 적절한 학습률을 골라야 함
- 배치 경사 하강법
    - 가용한 모든 데이터를 사용하여 반복을 실행함
    - 더 정확하지만, 더 비용이 많이 듬
    - 적절한 크기의 미니 배치를 사용하는 것이 효율적임
- 최적화 방법(옵티마이저)
    - 업데이트할 다음 가중치를 계산할 때 현제 그레이디언트 값 뿐만 아니라 이전에 업데이트된 가중치를 여러 방식으로 고려하는 SGD 변종들
    - ex) 모멘텀 사용 SGD, Adagrad 등
    - 모멘텀  
        - 작은 학습률을 가진 SGD는 지역 최솟값에 도달하면 전역 최솟값으로 향하지 못함
        - 작은 공을 곡선 위로 굴릴 때 이전에 받은 힘(모멘텀)이 충분하다면 곡선위로 올라감
        - 즉, 현재 기울기 값 뿐만 아니라 과거의 가속도로 인한 속도를 함께 고려하여 움직이는 것을 모멘텀이라 함
---
### 도함수 연결: 역전파 알고리즘
- 2개의 층을 가진 모델의 그레이디언트를 구하는 방법
- 연쇄 법칙
    - 신경망은 연결된 많은 텐서 연산으로 구성
    - 연산들은 간단, 도함수가 알려져 있음
    - 연쇄법칙을 통해 연결된 함수의 도함수를 구할 수 있음
    - 연쇄 법칙 : 합성함수를 미분할 때 사용
        - f(g(x))' = dy/dt * dt/dx = f'(g(x)) * g'(x)
        - f(g(h(x)))' = dy/dt * dt/dr * dr/dx = f'g(h(x)) * g'(h(x)) * h'(x)
- 계산 그래프를 활용한 자동 미분
