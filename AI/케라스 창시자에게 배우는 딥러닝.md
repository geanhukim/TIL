# 케라스 창시자에게 배우는 딥러닝
# 목차
- [1장 : 딥러닝이란 무엇인가?]()
    - [1.1 인공 지능과 머신 러닝, 딥러닝]
    - [1.2 딥러닝 이전: 머신 러닝의 간략한 역사]
    - [1.3 왜 딥러닝일까? 왜 지금일까?]
# 1장 : 딥러닝이란 무엇인가?
## 1.1 인공 지능과 머신 러닝, 딥러닝
### 인공지능
- '컴퓨터가 '생각'할 수 있는가?'라는 아이디어에서 시작
- 인공지능이란? 보통의 사람이 수행하는 지능적인 작업을 자동화하기 위한 연구 활동
- 머신러닝과 딥러닝을 포괄하는 종합 분야
- 심볼릭 AI : 명시적인 규칙을 충분히 만들어 db에 저장하여 지식을 다루는 방법
    - 1980년대까지 많이 쓰임
- 심볼릭 AI는 체스와 같이 잘 정의된 문제는 잘 풀었지만, 이미지 분류, 음성 인식과 같은 복잡하고 불분명한 문제를 해결하기 위한 정확한 규칙을 찾기 어려웠음 -> 머신 러닝 등장
### 머신 러닝
- 전통적인 프로그래밍에서는 프로그래머가 데이터를 적절한 해답으로 바꾸기 위해 따라야 하는 규칙을 작성함
- 머신러닝은 데이터와 이에 상응하는 해답을 보고 규칙을 찾아냄
### 데이터에서 표현을 학습하기
- 머신러닝을 하기 위해 필요한 것들
    - 입력 데이터 포인트
    - 기대 출력
    - 알고리즘의 성능을 측정하는 방법 : 현재 출력과 기대 출력 간 차이를 결정 => 학습
- ML/DL의 핵심 문제는 의미 있게 데이터를 변환하는 것 => 기대 출력에 가까워지도록 입력 데이터의 유용한 표현을 학습하는 것
- 표현 : 데이터를 인코딩하거나 표현하기 위해 데이터를 바라보는 방법
    - ex) 컬러 이미지를 RGB 포맷으로 볼 것인가, HSV 포맷으로 볼 것인가
### 딥러닝에서 '딥'이란 무엇일까?
- 딥러닝
    - 머신 러닝의 특정한 한 분야
    - 연속된 층(layer)에서 점진적으로 의미 있는 표현을 배우는데 강점
    - 딥러닝의 '딥' : 연속된 층으로 표현을 학습한다는 개념
- 신경망
    - 딥러닝이 사용하는 층이 겹겹이 쌓인 모델
### 딥러닝의 작동 원리 이해하기
- 가중치
    - 층이 입력 데이터를 처리하는 방식
    - 일련의 숫자로 이루어짐
    - 학습은 주어진 입력을 정확한 타깃에 매핑하기 위해 신경망의 모든 층에 있는 가중치의 값을 찾는 것
- 손실 함수(목적 함수, 비용 함수)
    - 신경망의 출력을 제어하기 위해 출력이 기대보다 얼머나 벗어났는지 측정
    - 신경망의 예측과 진짜 타깃의 차이를 점수로 계산
- 옵티마이저
    - 역전파 알고리즘을 사용하여 손실 점수가 감소되는 방향으로 가중치 값을 수정
- 훈련 반복
    - 초기에는 모델의 가중치가 랜덤값으로 할당
    - 자연스럽게 출력은 기대한 것과 달라지고, 손실점수가 매우 높음
    - 샘플 처리를 거듭하면서 가중치가 올바른 방향으로 조정, 손실 점수가 감소됨
### 지금까지 딥러닝의 성과
- 사람과 비슷한 수준의 이미지 분류, 음성 인식, 필기 인식
- 향상된 기계 번역, TTS 변환
- 디지털 비서
## 1.2 딥러닝 이전: 머신 러닝의 간략한 역사
### 확률적 모델링
- 통계학 이론을 데이터 분석에 응용
- 나이브 베이즈 알고리즘
    - 입력 데이터의 특성이 모두 독립적이라고 가정하고 베이즈 정리를 적용하는 머신 러닝 분류 알고리즘
- 로지스틱 회귀
### 초창기 신경망
- 1980년 대 중반부터 연구됨
- 경사 하강법 최적화를 사용하여 연쇄적으로 변수가 연결된 연산을 훈련하는 방법
### 커널 방법
- 분류 알고리즘의 한 종류
- 서포트 벡터 머신(SVM)
    - 두 클래스를 나누는 결정 경계를 찾는 분류 알고리즘
    - 과정
        - 결정 경계가 하나의 초평면으로 표현될 수 있는 고차원 표현으로 데이터를 매핑
        - 초평면과 각 클래스의 가장 가까운 데이터 포인트 사이의 거리가 최대가 되는 최선의 결정 경계를 찾음 -> 마진 최대화
### 결정 트리, 랜덤 포레스트, 그레이디언트 부스팅 머신
- 결정 트리
    - 입력 데이터 포인트를 분류하거나 주어진 입력에 대해 출력 값을 예측
- 랜덤 포레스터
    - 서로 다른 결정 트리를 많이 만들고 그 출력을 앙상블하는 방법
- 그레이디언트 부스팅 머신
    - 결정 트리를 앙상블하는 것을 기반으로 하는 머신 러닝 방법
    - 그레이디언트 부스팅 : 이전 모델에서 놓친 데이터 포인트를 보완하는 새로운 모델을 반복적으로 훈련하여 모델을 향상
### 다시 신경망으로
- 2011년 IDSIA에서 신층 신경망으로 학술 이미지 분류 대회에서 우승함으로써 시작
- 이후 심층 합성곱 신경망이 모든 컴퓨터 비전 작업의 주력 알고리즘으로 사용됨
### 딥러닝의 특징
- 딥러닝은 머신 러닝에서 가장 중요한 단계인 특성 공학을 완전히 자동화
- 특성 공학
    - 이전 머신 러닝 기법은 복잡한 문제를 풀 때 머신 러닝으로 처리하기 위해 입력 데이터를 수동으로 변환해야 함
    - 딥러닝은 직접 특성을 찾는 대신 한 번에 모든 특성을 학습함
- 
## 1.3 왜 딥러닝일까? 왜 지금일까?
# 2장 : 신경망의 수학적 구성요소
## 2.1 신경망과의 첫 만남
- MNIST 손글씨 숫자 이미지 분류
``` python
from tensorflow.keras.datasets import mnist
# 훈련 세트와 테스트 세트를 구성
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

from tensorflow import keras
from tensorflow.keras import layers
# 신경망 모델
model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(10, activation="softmax")
])

#컴파일 단계(옵티마이저 : 성능 향상을 위해 인풋을 기반으로 모델을 업데이트, 손실함수 : 모델의 성능을 측정, 모니터링 지표)
model.compile(optimizer="rmsprop", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

# 데이터를 모델에 맞는 크기로 조정
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype("float32") / 255

# 데이터에 모델을 학습
model.fit(train_images, train_labels, epochs=5, batch_size = 128)

# 모델 평가하기
test_loss, test_acc = model.evaluate(test_images, test_labels)
```
## 2.2 신경망을 위한 데이터 표현
- 텐서 : 데이터를 위한 컨테이너
### 스칼라(랭크-0 텐서)
- 하나의 숫자만 담고 있는 텐서
- 축 개수(랭크)가 0
```python
x = np.array(12)
x # array(12)
x.ndim # 0
```
### 벡터(랭크-1 텐서)
- 숫자의 배열
- 하나의 축을 가짐
```python
x = np.array([12, 3, 6, 14, 7])
x.ndim # 1
```
### 행렬(랭크-2 텐서)
- 벡터의 배열
- 2개의 축(행과 열)을 가짐
```python
x = np.array([
    [5, 78, 2, 34, 0],
    [6, 79, 3, 35, 1],
    [7, 89, 4, 36, 2]
])
x.ndim # 2
```
### 랭크-3 텐서와 더 높은 랭크의 텐서
- 행렬들을 하나의 배열로 합치면 랭크-3 텐서
- 랭크-3 텐서를 하나의 배열로 합치면 렝크-4 텐서
### 핵심 속성
- 축의 개수(랭크)
    - 랭크-3 텐서는 3개의 축, 행렬은 2개의 축
- 크기(shape)
    - 텐서의 각 축을 따라 얼마나 많은 차원이 있는지를 나타낸 파이썬의 튜플
    - 위의 행렬의 크기는 (3, 5), 벡터의 크기는 (5,), 스칼라는 ()
- 데이터 타입
    - 텐서의 포함된 데이터의 타입
### 넘파이로 텐서 조작하기
- 슬라이싱 : 배열에 있는 특정 원소들을 선택하는 것
### 배치 데이터
- 샘플 축 : 딥러닝에서 사용하는 모든 데이터 텐서의 첫 번째 축
### 텐서의 실제 사례
- 벡터 데이터
    - 첫 번째 축 : 샘플 축
    - 두 번째 축 : 특성 축
    - ex) 3개의 특성을 가진 10만명의 인구 통계 데이터 : (100000, 3)
- 시계열 또는 시퀀스 데이터
    - 시간 축을 포함하여 랭크-3 텐서로 저장
    - 시간 축은 관례적으로 두 번째 축
- 이미지 데이터
    - 관례상 랭크-3 텐서로 저장
    - ex) 128 개의 256 * 256크기의 컬러 이미지 : (128, 256, 256, 2)
- 비디오 데이터
    - 랭크-5 텐서
## 2.3 신경망의 톱니바퀴: 텐서 연산
### 원소별 연산
- 원소별로 덧셈, 뺄셈, 곱셈 등을 하는 것
- 넘파이 배열을 다룰 때 넘파이 내장함수로 더 빠르게 처리할 수 있음
### 브로드캐스팅
- 다른 랭크의 텐서끼리의 연산에서 작은 텐서를 큰 텐서의 크기에 맞추어 변환 후 계산하는 것
### 텐서 곱셈(점곱)
- `np.dot`함수를 사용
### 텐서 크기 변환
-  특정 크기에 맞게 열과 행을 재배열
### 텐서 연산의 기하학적 해석
- 